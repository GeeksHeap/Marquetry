# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Little Tabby
# This file is distributed under the same license as the Marquetry package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Marquetry v0.2.0-dev\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-10-20 11:24+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: ja\n"
"Language-Team: ja <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../functions/normalizations/layer_normalization.rst:3
msgid "Layer Normalization"
msgstr ""

#: marquetry.functions.normalization.layer_normalization.LayerNormalization:1
#: marquetry.functions.normalization.layer_normalization.layer_normalization:1
#: of
msgid "Apply layer normalization to the input container."
msgstr ""

#: marquetry.functions.normalization.layer_normalization.LayerNormalization:3
#: marquetry.functions.normalization.layer_normalization.layer_normalization:3
#: of
msgid ""
"Layer normalization is a technique used in deep neural networks to "
"stabilize and accelerate training. It normalizes the input data by "
"normalization and  adjustable parameters called gamma(scale factor) and "
"beta(shift factor). This doesn't depend on the batch size due to "
"normalize by the only 1 layer values. In some use cases, the stability is"
" better than :class:`marquetry.functions.BatchNormalization`. This helps "
"in preventing issues like vanishing gradients, reduce co-variate shift, "
"and allow for faster convergence."
msgstr ""

#: marquetry.functions.normalization.layer_normalization.LayerNormalization.backward
#: marquetry.functions.normalization.layer_normalization.LayerNormalization.forward
#: marquetry.functions.normalization.layer_normalization.layer_normalization of
msgid "Parameters"
msgstr ""

#: marquetry.functions.normalization.layer_normalization.layer_normalization:11
#: of
msgid "The input tensor."
msgstr ""

#: marquetry.functions.normalization.layer_normalization.layer_normalization:13
#: of
msgid ""
"The scale factor. It allows the network to learn the optimal scaling for "
"each feature."
msgstr ""

#: marquetry.functions.normalization.layer_normalization.layer_normalization:15
#: of
msgid ""
"The shift factor. It allows the network to learn the optimal mean for "
"each feature."
msgstr ""

#: marquetry.functions.normalization.layer_normalization.layer_normalization:17
#: of
msgid "A small value to prevent division by zero. Default is 1e-15."
msgstr ""

#: marquetry.functions.normalization.layer_normalization.layer_normalization:23
#: of
msgid ""
"Generally use case, you can use LayerNormalization in "
":mod:`marquetry.layers`. The layer components manage the params like "
"gamma, beta itself. Therefore, we suggest to use it for your network if "
"you have no special meaning to use this function."
msgstr ""

#: marquetry.functions.normalization.layer_normalization.LayerNormalization.backward
#: marquetry.functions.normalization.layer_normalization.LayerNormalization.forward
#: marquetry.functions.normalization.layer_normalization.layer_normalization of
msgid "Returns"
msgstr ""

#: marquetry.functions.normalization.layer_normalization.layer_normalization:27
#: of
msgid "The normalized and scaled input container."
msgstr ""

#: marquetry.functions.normalization.layer_normalization.LayerNormalization.backward
#: marquetry.functions.normalization.layer_normalization.LayerNormalization.forward
#: marquetry.functions.normalization.layer_normalization.layer_normalization of
msgid "Return type"
msgstr ""

#: marquetry.functions.normalization.layer_normalization.layer_normalization:31
#: of
msgid "References"
msgstr ""

#: marquetry.functions.normalization.layer_normalization.layer_normalization:32
#: of
msgid "Layer Normalization (https://arxiv.org/abs/1607.06450)"
msgstr ""

#: marquetry.functions.normalization.layer_normalization.LayerNormalization:1
#: of
msgid "Bases: :py:class:`~marquetry.function.Function`"
msgstr ""

#: marquetry.functions.normalization.layer_normalization.LayerNormalization:13
#: of
msgid ""
"Generally, you don't need to execute ``forward`` and ``backward`` method "
"manually. You should use only ``__call__`` method."
msgstr ""

#: marquetry.functions.normalization.layer_normalization.LayerNormalization.backward:1
#: of
msgid "Perform the backward computation of the function."
msgstr ""

#: marquetry.functions.normalization.layer_normalization.LayerNormalization.backward:3
#: of
msgid "Gradient data arrays."
msgstr ""

#: marquetry.functions.normalization.layer_normalization.LayerNormalization.backward:6
#: of
msgid "Gradient data arrays with respect to the input data arrays."
msgstr ""

#: marquetry.functions.normalization.layer_normalization.LayerNormalization.backward:11
#: of
msgid ""
"Function backward should be called by only marquetry (user shouldn't call"
" this method). Generally, a user should call the backward method in "
":class:`marquetry.Container` of the model output."
msgstr ""

#: marquetry.functions.normalization.layer_normalization.LayerNormalization.forward:1
#: of
msgid "Perform the forward computation of the function."
msgstr ""

#: marquetry.functions.normalization.layer_normalization.LayerNormalization.forward:3
#: of
msgid "Input data arrays."
msgstr ""

#: marquetry.functions.normalization.layer_normalization.LayerNormalization.forward:6
#: of
msgid "Output data arrays."
msgstr ""

#: marquetry.functions.normalization.layer_normalization.LayerNormalization.forward:9
#: of
msgid ""
"Generally, this class shouldn't be called by manually because `forward` "
"is called via `__call__`."
msgstr ""

